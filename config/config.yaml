data:
  raw_csv_path: "data/raw/twcs_cleaned.csv"
  processed_parquet_dir: "data/processed/"
  sample_csv_path: "data/sample/twcs_sample.csv"
  timestamp_column: "created_at"
  text_column: "text"
  inbound_column: "inbound"

model:
  embedding_model: "all-MiniLM-L6-v2"
  min_cluster_size: 3  # Reduced for small datasets
  min_samples: 2  # Reduced for small datasets
  umap_n_components: 5
  umap_n_neighbors: 5  # Reduced for small datasets
  umap_min_dist: 0.0
  umap_metric: "cosine"
  hdbscan_metric: "euclidean"
  min_df: 1  # Allow all words for small datasets
  max_df: 1.0  # Allow all words for small datasets
  ngram_range: [1, 2]
  top_n_words: 10  # Number of top words per topic

storage:
  topics_metadata_path: "outputs/topics/topics_metadata.json"
  doc_assignments_path: "outputs/assignments/doc_assignments.csv"
  alerts_path: "outputs/alerts/drift_alerts.csv"
  audit_log_path: "outputs/audit/hitl_audit_log.csv"
  batch_log_path: "outputs/audit/batch_run_log.csv"
  current_model_path: "models/current/bertopic_model.pkl"
  previous_model_path: "models/previous/bertopic_model.pkl"
  state_file: "data/state/processing_state.json"
  
mlflow:
  tracking_uri: "file:./mlruns"
  experiment_name: "twcs_topic_modeling"

ollama:
  enabled: true
  base_url: "http://localhost:11434"
  model: "gemma3:1b"  # Change to your local model name
  temperature: 0.2
  max_tokens: 128
  timeout_seconds: 30
  examples_limit: 3
  prompt_template: |
    You are labeling support-ticket topics.
    Given keywords and example texts, return JSON with keys: label, summary.
    Label: 3-6 words. Summary: one sentence.

    Keywords: {keywords}
    Examples:
    {examples}

api:
  host: "0.0.0.0"
  port: 8000
  cors_origins:
    - "http://localhost:8501"
    - "http://127.0.0.1:8501"

dashboard:
  host: "0.0.0.0"
  port: 8501
  api_base_url: "http://localhost:8000"

scheduler:
  batch_size: 5000
  window_minutes: 5
  schedule_cron: "*/5 * * * *"  # Every 5 minutes

prefect:
  api_url: "http://127.0.0.1:4200/api"
  logging_level: "INFO"
  work_queue: "default"
  work_pool: "default-agent-pool"
  flow_run_name_template: "pipeline-{date}"
  task_retry_delay_seconds: 10
  max_retries: 2
  storage_path: "data/prefect"

