# TwCS Online Topic Modeling System

An end-to-end online topic modeling system for Twitter Customer Support (TwCS) conversations using BERTopic, with a FastAPI backend and Streamlit dashboard.

## Features

- ðŸ“Š **Online Topic Modeling**: Train and update BERTopic models incrementally
- ðŸ”„ **Drift Detection**: Monitor topic evolution with prevalence, centroid shift, and keyword divergence metrics
- ðŸŽ¯ **Real-time Inference**: Predict topics for new customer support messages
- âœï¸ **Human-in-the-Loop**: Merge, split, and relabel topics with full audit trail
- ðŸ“ˆ **Interactive Dashboard**: Visualize topics, trends, and drift alerts
- ðŸ¤– **MLflow Integration**: Track model versions and experiments
- âš™ï¸ **Automated Pipeline**: Scheduled batch processing with state management

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Layer    â”‚  TwCS CSV â†’ Processed Parquet
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ETL Pipeline  â”‚  Clean â†’ Filter â†’ Transform
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Modeling Layer â”‚  Seed Model â†’ Online Updates
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Drift Detection â”‚  Prevalence â†’ Centroid â†’ Keywords
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Backendâ”‚  REST API + Model Serving
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Streamlit Dashboardâ”‚  UI + Visualizations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.9+
- TwCS dataset (CSV format)

### One-Command Startup (NEW!)

The easiest way to start the **complete system** with all components:

```bash
# Single script to start everything (API + Dashboard + Prefect)
./run_full_system.sh

# Choose from:
# 1) API + Dashboard only (Simple)
# 2) API + Dashboard + Prefect (Full System) â­ Recommended
# 3) Prefect only (for scheduling)
# 4) Exit (manual startup)
```

### Manual Installation

1. **Clone the repository**
```bash
cd "/Users/krishnagajera/Project/Final Year Project/Master Project-BERTopic"
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Place TwCS dataset**
```bash
# Place your TwCS dataset in data/raw/twcs.csv
# Or create a sample:
cp your_twcs_data.csv data/raw/twcs.csv

# For testing, create a sample (first 50k rows):
head -n 50001 data/raw/twcs.csv > data/sample/twcs_sample.csv
```

### Running the System

#### 1. Train Initial Model (One-time Setup)

```bash
# Run initial training with sample data
python src/scheduler/run_window.py --init
```

This will:
- Process the first batch of data
- Train the seed BERTopic model
- Save model artifacts and metadata
- Initialize processing state

#### 2. Start FastAPI Backend

```bash
# In terminal 1
python -m src.api.main
```

API will be available at: `http://localhost:8000`
API docs at: `http://localhost:8000/docs`

#### 3. Start Streamlit Dashboard

```bash
# In terminal 2
streamlit run src/dashboard/app.py
```

Dashboard will open at: `http://localhost:8501`

#### 4. Run Scheduled Updates with Prefect

**Option A: Using Prefect (Recommended for Production)**

```bash
# Start Prefect server and agent
./scripts/start_prefect.sh

# Deploy flows (one-time)
./scripts/deploy_flows.sh

# View Prefect UI
open http://127.0.0.1:4200

# Run pipeline manually through Prefect
./scripts/run_pipeline_manual.sh

# Stop Prefect when done
./scripts/stop_prefect.sh
```

**Option B: Using Simple Scheduler (Quick Testing)**

```bash
# Run window pipeline manually
python src/scheduler/run_window.py --window

# Or set up cron job for daily updates:
crontab -e
# Add: 0 2 * * * cd /path/to/project && /path/to/venv/bin/python src/scheduler/run_window.py --window
```

## Project Structure

```
Master-Project-BERTopic/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl.py                    # Data ingestion and preprocessing
â”‚   â”œâ”€â”€ modeling.py               # BERTopic training and updates
â”‚   â”œâ”€â”€ drift.py                  # Drift detection algorithms
â”‚   â”œâ”€â”€ storage.py                # Data persistence layer
â”‚   â”œâ”€â”€ api/                      # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ main.py              # API application
â”‚   â”‚   â”œâ”€â”€ endpoints/           # API routes
â”‚   â”‚   â””â”€â”€ models/              # Pydantic schemas
â”‚   â”œâ”€â”€ dashboard/               # Streamlit frontend
â”‚   â”‚   â”œâ”€â”€ app.py              # Main dashboard
â”‚   â”‚   â”œâ”€â”€ pages/              # Multi-page app
â”‚   â”‚   â””â”€â”€ utils/              # API client
â”‚   â”œâ”€â”€ scheduler/              # Simple batch processing
â”‚   â”‚   â””â”€â”€ run_window.py      # Basic scheduler (cron)
â”‚   â””â”€â”€ utils/                  # Shared utilities
â”œâ”€â”€ etl/                        # Prefect orchestration
â”‚   â”œâ”€â”€ tasks/                 # Prefect tasks
â”‚   â”‚   â”œâ”€â”€ data_tasks.py     # ETL tasks
â”‚   â”‚   â”œâ”€â”€ model_tasks.py    # Model training tasks
â”‚   â”‚   â””â”€â”€ drift_tasks.py    # Drift detection tasks
â”‚   â”œâ”€â”€ flows/                 # Prefect flows
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py        # Data pipeline
â”‚   â”‚   â”œâ”€â”€ model_training.py        # Training pipeline
â”‚   â”‚   â”œâ”€â”€ drift_detection.py       # Drift pipeline
â”‚   â”‚   â””â”€â”€ complete_pipeline.py     # Master flow
â”‚   â””â”€â”€ schedules/             # Deployment configs
â”‚       â”œâ”€â”€ deploy.py         # Deploy flows
â”‚       â””â”€â”€ prefect_config.py # Prefect settings
â”œâ”€â”€ scripts/                    # Helper scripts
â”‚   â”œâ”€â”€ start_prefect.sh      # Start Prefect
â”‚   â”œâ”€â”€ stop_prefect.sh       # Stop Prefect
â”‚   â”œâ”€â”€ deploy_flows.sh       # Deploy flows
â”‚   â””â”€â”€ run_pipeline_manual.sh # Manual run
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw CSV files
â”‚   â”œâ”€â”€ processed/              # Processed Parquet
â”‚   â””â”€â”€ sample/                 # Sample data for testing
â”œâ”€â”€ models/                     # Trained models
â”‚   â”œâ”€â”€ current/               # Active model
â”‚   â””â”€â”€ previous/              # Previous version
â”œâ”€â”€ outputs/                    # Results
â”‚   â”œâ”€â”€ topics/                # Topic metadata (JSON)
â”‚   â”œâ”€â”€ assignments/           # Doc-topic assignments (CSV)
â”‚   â”œâ”€â”€ alerts/                # Drift alerts (CSV)
â”‚   â””â”€â”€ audit/                 # HITL audit log (CSV)
â”œâ”€â”€ logs/                       # Application logs
â”œâ”€â”€ config/                     # Configuration files
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## API Endpoints

### Topics
- `GET /api/v1/topics/current` - Get all current topics
- `GET /api/v1/topics/{topic_id}` - Get topic details
- `GET /api/v1/topics/{topic_id}/examples` - Get example documents

### Trends
- `GET /api/v1/trends` - Get topic trends over time

### Alerts
- `GET /api/v1/alerts` - Get drift alerts
- `GET /api/v1/alerts/latest` - Get latest alert

### Inference
- `POST /api/v1/infer` - Predict topic for text

### HITL (Human-in-the-Loop)
- `POST /api/v1/hitl/merge` - Merge topics
- `POST /api/v1/hitl/relabel` - Relabel topic
- `GET /api/v1/hitl/audit` - Get audit log

### Pipeline
- `GET /api/v1/pipeline/status` - Get pipeline status

## Dashboard Pages

1. **Home** - Overview and quick statistics
2. **Topics & Trends** - View all topics and their evolution
3. **Topic Drill-down** - Deep dive into specific topics
4. **Drift Alerts** - Monitor topic drift and changes
5. **HITL Editor** - Merge, relabel topics with audit trail
6. **Inference** - Predict topics for new text

## Configuration

Edit `config/config.yaml` to customize:
- Data paths
- Model hyperparameters
- API/Dashboard settings
- Batch processing parameters
- MLflow tracking

Edit `config/drift_thresholds.yaml` for drift detection sensitivity.

## Prefect Orchestration

The system includes two orchestration options:

### 1. Prefect (Recommended for Production)

**Features:**
- Web UI for monitoring at `http://127.0.0.1:4200`
- Task retry logic and error handling
- Schedule management (daily/weekly)
- Flow run history and logging
- Distributed task execution (with Dask)

**Setup:**
```bash
# Install Prefect (already in requirements.txt)
pip install prefect>=2.14.0

# Start Prefect server + agent
./scripts/start_prefect.sh

# Deploy flows with schedules
./scripts/deploy_flows.sh
```

**Scheduled Deployments:**
- `daily-pipeline`: Runs at 2 AM every day
- `weekly-pipeline`: Runs at 3 AM every Sunday

**Manual Execution:**
```bash
# Run through Prefect UI or CLI
prefect deployment run complete-pipeline-flow/daily-pipeline

# Or use helper script
./scripts/run_pipeline_manual.sh
```

**Monitoring:**
Visit `http://127.0.0.1:4200` to:
- View flow runs and task status
- Check logs and errors
- Monitor schedules
- Trigger manual runs

**Stop Prefect:**
```bash
./scripts/stop_prefect.sh
```

### 2. Simple Scheduler (For Testing/Development)

If you don't need Prefect's advanced features, use the simple scheduler:

```bash
# Manual run
python src/scheduler/run_window.py --window

# Or via cron
crontab -e
# Add: 0 2 * * * cd /path/to/project && /path/to/venv/bin/python src/scheduler/run_window.py --window
```

## Workflow

### Initial Training
1. Load historical TwCS data
2. Clean and filter customer tweets
3. Train seed BERTopic model
4. Extract and save topic metadata

### Online Updates
1. Scheduler fetches new batch of tweets
2. ETL pipeline processes data
3. Model updates with new batch
4. Drift detection compares models
5. Alerts generated if thresholds exceeded
6. Dashboard auto-updates

### HITL Refinement
1. User reviews topics in dashboard
2. Merge similar topics
3. Relabel with meaningful names
4. Changes logged in audit trail

## Development

### Run Tests
```bash
pytest tests/
```

### Code Structure
- **ETL**: Handles data ingestion, cleaning, filtering
- **Modeling**: BERTopic wrapper with online learning
- **Drift**: Multi-metric drift detection
- **Storage**: Centralized data persistence
- **API**: RESTful backend with FastAPI
- **Dashboard**: Interactive Streamlit UI

## Troubleshooting

### API Connection Error
- Ensure FastAPI is running: `python -m src.api.main`
- Check port 8000 is not in use

### No Topics Found
- Run initial setup: `python src/scheduler/run_window.py --init`
- Check `models/current/` for model file

### Import Errors
- Activate virtual environment: `source venv/bin/activate`
- Install dependencies: `pip install -r requirements.txt`

### Memory Issues
- Reduce batch size in `config/config.yaml`
- Use sample data for testing

## Technologies

- **BERTopic**: Topic modeling with transformers
- **FastAPI**: Modern Python web framework
- **Streamlit**: Interactive dashboards
- **MLflow**: Experiment tracking
- **Pandas**: Data processing
- **Plotly**: Interactive visualizations
- **scikit-learn**: ML utilities
- **UMAP**: Dimensionality reduction
- **HDBSCAN**: Density-based clustering

## License

MIT License

## Citation

If you use this system in your research, please cite:

```bibtex
@software{twcs_topic_modeling_2024,
  title={TwCS Online Topic Modeling System},
  author={Krishna Gajera},
  year={2024},
  url={https://github.com/yourusername/twcs-topic-modeling}
}
```

## Contact

For questions or issues, please open an issue on GitHub or contact Krishna Gajera.

---

**Note**: This is a prototype system. For production deployment, consider:
- Database instead of CSV/JSON storage
- Proper authentication and authorization
- Load balancing for API
- Containerization with Docker
- CI/CD pipeline
- Monitoring and alerting
